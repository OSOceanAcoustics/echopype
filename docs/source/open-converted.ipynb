{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(function:open-converted)=\n",
    "# Working with converted files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open a converted netCDF or Zarr dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converted netCDF files can be opened with the [`open_converted`](echopype.open_converted) function that returns a lazy-loaded [`EchoData` object](data-format:echodata-object) (only metadata are read during opening):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import echopype as ep\n",
    "file_path = \"./converted_files/file.nc\"      # path to a converted nc file\n",
    "ed = ep.open_converted(file_path)            # create an EchoData object\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, specify the path to open a Zarr dataset. To open such a dataset from cloud storage, use the same `storage_options` parameter as with [open_raw](convert.html#aws-s3-access). For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "s3_path = \"s3://s3bucketname/directory_path/dataset.zarr\"     # S3 dataset path\n",
    "ed = ep.open_converted(s3_path, storage_options={\"anon\": True})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine EchoData objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converted data found in multiple files corresponding to the same instrument deployment can be combined into a single [`EchoData` object](data-format:echodata-object) using [`combine_echodata`](echopype.combine_echodata). With the release of `echopype` version `0.6.3`, one can now combine a large number of files in parallel (using [Dask](https://www.dask.org/)) while maintaining a stable memory usage. To accomplish this, each `EchoData` object is directly appended to a Zarr store, which corresponds to the final combined `EchoData` object. The path of this Zarr store is determined by the keyword argument `zarr_path`. In addition to this argument, `combine_echodata` also accepts the keyword argument `client`, which represents an initialized Dask distributed client that parallel tasks will be submitted to. If `zarr_path` or `client` are not provided, then default values will be used (see the `Notes` section in [`combine_echodata`](echopype.combine_echodata)).\n",
    "\n",
    "To use `combine_echodata`, the following criteria must be met: \n",
    "- Each `EchoData` object must have the same `sonar_model`\n",
    "- Each `EchoData` object must be produced by distinct file paths\n",
    "- The first time value of each `EchoData` group must be less than the first time value of the subsequent corresponding `EchoData` group, with respect to the order in the list of `EchoData` objects being combined\n",
    "- The same `EchoData` groups in the list of `EchoData` objects must have the same number of channels and the same name for each of these channels\n",
    "- The following attribute criteria must be satisfied amongst the combined `EchoData` groups:\n",
    "  - the names of each attribute must be the same\n",
    "  - the values of each attribute must be identical (other than the attributes `date_created` or `conversion_time` which must have the same types)\n",
    "\n",
    "## Combining converted files\n",
    "\n",
    "The first step in combining converted files is to establish a Dask client with a scheduler. If one is working on a local machine, this can be done as follows:\n",
    "```python\n",
    "client = Client()  # create client with local scheduler\n",
    "```\n",
    "If one is interested in running on distributed hardware, we highly suggest reviewing the Dask documentation for [deploying Dask clusters](https://docs.dask.org/en/latest/deploying.html). \n",
    "\n",
    "Next, we assemble a list of `EchoData` objects from converted files (netCDF or Zarr):\n",
    "```python\n",
    "ed_list = []\n",
    "for converted_file in [\"convertedfile1.zarr\", \"convertedfile2.zarr\"]:\n",
    "    ed_list.append(ep.open_converted(converted_file))\n",
    "```\n",
    "\n",
    "Finally, we apply `combine_echodata` on this list to combine all the data into a single `EchoData` object. Here, we will store the final combined form in the Zarr path `path_to/combined_echodatas.zarr` and use the client we established above: \n",
    "```python\n",
    "combined_ed = ep.combine_echodata(ed_list, \n",
    "                                  zarr_path='path_to/combined_echodatas.zarr', \n",
    "                                  client=client)\n",
    "```\n",
    "Once executed, `combine_echodata` returns a lazy loaded `EchoData` object (obtained from `zarr_path`) with all data from the input `EchoData` objects combined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{Note}\n",
    "In previous versions, `combine_echodata` corrected reversed timestamps and stored the uncorrected timestamps in the `Provenance` group. The current implementation of `combine_echodata` now allows us to preserve time coordinates that have reversed timestamps and we no longer perform this correction. \n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "a292767406182d99a2458e67c2d2e96b524510c4a2166b4b423439fe75c32190"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
