{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(function:open-converted)=\n",
        "# Working with converted files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Open a converted netCDF or Zarr dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Converted netCDF files can be opened with the [`open_converted`](echopype.open_converted) function that returns a lazy-loaded [`EchoData` object](data-format:echodata-object) (only metadata are read during opening):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```python\n",
        "import echopype as ep\n",
        "file_path = \"./converted_files/file.nc\"      # path to a converted nc file\n",
        "ed = ep.open_converted(file_path)            # create an EchoData object\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Likewise, specify the path to open a Zarr dataset. To open such a dataset from cloud storage, use the same `storage_options` parameter as with [open_raw](convert.html#aws-s3-access). For example:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```python\n",
        "s3_path = \"s3://s3bucketname/directory_path/dataset.zarr\"     # S3 dataset path\n",
        "ed = ep.open_converted(s3_path, storage_options={\"anon\": True})\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Combine EchoData objects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data collected by the same instrument deployment across multiple files can be combined into a single [`EchoData` object](data-format:echodata-object) using [`combine_echodata`](echopype.combine_echodata). With the release of echopype version `0.6.3`, one can now combine a large number of files in parallel (using [Dask](https://www.dask.org/)) while maintaining a stable memory usage. This is done under-the-hood by concatenating data directly into a Zarr store, which corresponds to the final combined `EchoData` object. \n",
        "\n",
        "To use `combine_echodata`, the following criteria must be met: \n",
        "- Each `EchoData` object must have the same `sonar_model`\n",
        "- The `EchoData` objects to be combined must correspond to different raw data files (i.e., no duplicated files)\n",
        "- The `EchoData` objects in the list must be of sequential order in time. Specifically, the first timestamp of each `EchoData` object must be smaller (earlier) than the first timestamp of the subsequent `EchoData` object\n",
        "- The `EchoData` objects must contain the same frequency channels and the same number of channels\n",
        "- The following attribute criteria must be satisfied for all groups under each of the `EchoData` objects to be combined:\n",
        "  - the names of all attributes must be the same\n",
        "  - the values of all attributes must be identical (other than the attributes `date_created` or `conversion_time`; these attributes should have the same data type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::{Attention}\n",
        "In previous versions, `combine_echodata` corrected reversed timestamps and stored the uncorrected timestamps in the `Provenance` group.\n",
        "Starting from `0.6.3`, `combine_echodata` will preserve time coordinates that have reversed timestamps and not correction is performed. \n",
        ":::"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "The first step in combining data is to establish a Dask client with a scheduler. On a local machine, this can be done as follows:\n",
        "```python\n",
        "client = Client()  # create client with local scheduler\n",
        "```\n",
        "With distributed resources, we highly recommend reviewing the Dask documentation for [deploying Dask clusters](https://docs.dask.org/en/latest/deploying.html). \n",
        "\n",
        "Next, we assemble a list of `EchoData` objects. This list can be from converted files (netCDF or Zarr) as in the example below, or from in-memory `EchoData` objects:\n",
        "```python\n",
        "ed_list = []\n",
        "for converted_file in [\"convertedfile1.zarr\", \"convertedfile2.zarr\"]:\n",
        "    ed_list.append(ep.open_converted(converted_file))  # already converted files are lazy-loaded\n",
        "```\n",
        "\n",
        "Finally, we apply `combine_echodata` on this list to combine all the data into a single `EchoData` object. Here, we will store the final combined form in the Zarr path `path_to/combined_echodata.zarr` and use the client we established above: \n",
        "```python\n",
        "combined_ed = ep.combine_echodata(\n",
        "    ed_list, \n",
        "    zarr_path='path_to/combined_echodata.zarr', \n",
        "    client=client\n",
        ")\n",
        "```\n",
        "Once executed, `combine_echodata` returns a lazy loaded `EchoData` object (obtained from `zarr_path`) with all data from the input `EchoData` objects combined."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::{Note}\n",
        "As shown in the above example, the path of the combined Zarr store is given by the keyword argument `zarr_path`, \n",
        "and the Dask client that parallel tasks will be submitted to is given by the keyword argument `client`.\n",
        "When either (or both) of these are not provided, default values listed in the `Notes` section in [`combine_echodata`](echopype.combine_echodata) will be used.\n",
        ":::"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Tags",
    "interpreter": {
      "hash": "a292767406182d99a2458e67c2d2e96b524510c4a2166b4b423439fe75c32190"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
