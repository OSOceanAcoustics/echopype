scheduler:
  address: tcp://127.0.0.1:50971
  clients:
    Client-854fe396-3b63-11ed-b660-7aef93c2516e:
      client_key: Client-854fe396-3b63-11ed-b660-7aef93c2516e
      last_seen: 1663953414.2823439
      wants_what: []
    fire-and-forget:
      client_key: fire-and-forget
      last_seen: 1663953414.2209349
      wants_what: []
  events:
    Client-854fe396-3b63-11ed-b660-7aef93c2516e:
    - - 1663953414.282331
      - action: add-client
        client: Client-854fe396-3b63-11ed-b660-7aef93c2516e
    all:
    - - 1663953414.261132
      - action: add-worker
        worker: tcp://127.0.0.1:50972
    - - 1663953414.262537
      - action: add-worker
        worker: tcp://127.0.0.1:50974
    - - 1663953414.282331
      - action: add-client
        client: Client-854fe396-3b63-11ed-b660-7aef93c2516e
    stealing: []
    tcp://127.0.0.1:50972:
    - - 1663953414.261111
      - action: add-worker
    - - 1663953414.265065
      - action: worker-status-change
        prev-status: init
        status: running
    tcp://127.0.0.1:50974:
    - - 1663953414.262531
      - action: add-worker
    - - 1663953414.2653031
      - action: worker-status-change
        prev-status: init
        status: running
  extensions:
    amm: <distributed.active_memory_manager.ActiveMemoryManagerExtension object at
      0x7fd9a2a23070>
    events: <distributed.event.EventExtension object at 0x7fd99102afa0>
    locks: <distributed.lock.LockExtension object at 0x7fd99102aa30>
    memory_sampler: <distributed.diagnostics.memory_sampler.MemorySamplerExtension
      object at 0x7fd9a2a23100>
    multi_locks: <distributed.multi_lock.MultiLockExtension object at 0x7fd99102ad30>
    publish: <distributed.publish.PublishExtension object at 0x7fd99102aca0>
    pubsub: <distributed.pubsub.PubSubSchedulerExtension object at 0x7fd99102ae20>
    queues: <distributed.queues.QueueExtension object at 0x7fd99102adc0>
    replay-tasks: <distributed.recreate_tasks.ReplayTaskScheduler object at 0x7fd99102ad00>
    semaphores: <distributed.semaphore.SemaphoreExtension object at 0x7fd99102af70>
    shuffle: <distributed.shuffle.shuffle_extension.ShuffleSchedulerExtension object
      at 0x7fd9a2a23160>
    stealing:
      cost_multipliers:
      - 1.0
      - 1.03125
      - 1.0625
      - 1.125
      - 1.25
      - 1.5
      - 2
      - 3
      - 5
      - 9
      - 17
      - 33
      - 65
      - 129
      - 257
      count: 0
      in_flight: {}
      in_flight_occupancy: {}
      key_stealable: {}
      scheduler:
        address: tcp://127.0.0.1:50971
        clients:
          Client-854fe396-3b63-11ed-b660-7aef93c2516e: <Client 'Client-854fe396-3b63-11ed-b660-7aef93c2516e'>
          fire-and-forget: <Client 'fire-and-forget'>
        events:
          Client-854fe396-3b63-11ed-b660-7aef93c2516e:
          - - 1663953414.282331
            - action: add-client
              client: Client-854fe396-3b63-11ed-b660-7aef93c2516e
          all:
          - - 1663953414.261132
            - action: add-worker
              worker: tcp://127.0.0.1:50972
          - - 1663953414.262537
            - action: add-worker
              worker: tcp://127.0.0.1:50974
          - - 1663953414.282331
            - action: add-client
              client: Client-854fe396-3b63-11ed-b660-7aef93c2516e
          stealing: []
          tcp://127.0.0.1:50972:
          - - 1663953414.261111
            - action: add-worker
          - - 1663953414.265065
            - action: worker-status-change
              prev-status: init
              status: running
          tcp://127.0.0.1:50974:
          - - 1663953414.262531
            - action: add-worker
          - - 1663953414.2653031
            - action: worker-status-change
              prev-status: init
              status: running
        extensions: '{''locks'': <distributed.lock.LockExtension object at 0x7fd99102aa30>,
          ''multi_locks'': <distributed.multi_lock.MultiLockExtension object at 0x7fd99102ad30>,
          ''publish'': <distributed.publish.PublishExtension object at 0x7fd99102aca0>,
          ''replay-tasks'': <distributed.recreate_tasks.ReplayTaskScheduler object
          at 0x7fd99102ad00>, ''queues'': <distributed.queues.QueueExtension object
          at 0x7fd99102adc0>, ''variables'': <distributed.variable.VariableExtension
          object at 0x7fd99102aeb0>, ''pubsub'': <distributed.pubsub.PubSubSchedulerExtension
          object at 0x7fd99102ae20>, ''semaphores'': <distributed.semaphore.SemaphoreExtension
          object at 0x7fd99102af70>, ''events'': <distributed.event.EventExtension
          object at 0x7fd99102afa0>, ''amm'': <distributed.active_memory_manager.ActiveMemoryManagerExtension
          object at 0x7fd9a2a23070>, ''memory_sampler'': <distributed.diagnostics.memory_sampler.MemorySamplerExtension
          object at 0x7fd9a2a23100>, ''shuffle'': <distributed.shuffle.shuffle_extension.ShuffleSchedulerExtension
          object at 0x7fd9a2a23160>, ''stealing'': <distributed.stealing.WorkStealing
          object at 0x7fd9a2a23130>}'
        id: Scheduler-6409852a-5ae7-46a1-956a-da3f1329a529
        log: []
        memory:
          managed: 0
          managed_in_memory: 0
          managed_spilled: 0
          optimistic: 388939776
          process: 388939776
          unmanaged: 388939776
          unmanaged_old: 388939776
          unmanaged_recent: 0
        services:
          dashboard: 50970
        started: 1663953414.037181
        status: running
        task_groups: {}
        tasks: {}
        thread_id: 8633697792
        transition_counter: 0
        transition_log: []
        type: Scheduler
        workers:
          tcp://127.0.0.1:50972: '<WorkerState ''tcp://127.0.0.1:50972'', name: 0,
            status: running, memory: 0, processing: 0>'
          tcp://127.0.0.1:50974: '<WorkerState ''tcp://127.0.0.1:50974'', name: 1,
            status: running, memory: 0, processing: 0>'
      stealable:
        tcp://127.0.0.1:50972:
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        tcp://127.0.0.1:50974:
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
        - []
      stealable_all:
      - []
      - []
      - []
      - []
      - []
      - []
      - []
      - []
      - []
      - []
      - []
      - []
      - []
      - []
      - []
    variables: <distributed.variable.VariableExtension object at 0x7fd99102aeb0>
  id: Scheduler-6409852a-5ae7-46a1-956a-da3f1329a529
  log: []
  memory:
    managed: 0
    managed_in_memory: 0
    managed_spilled: 0
    optimistic: 388939776
    process: 388939776
    unmanaged: 388939776
    unmanaged_old: 388939776
    unmanaged_recent: 0
  services:
    dashboard: 50970
  started: 1663953414.037181
  status: running
  task_groups: {}
  tasks: {}
  thread_id: 8633697792
  transition_counter: 0
  transition_log: []
  type: Scheduler
  workers:
    tcp://127.0.0.1:50972:
      actors: []
      address: tcp://127.0.0.1:50972
      bandwidth: 100000000
      executing: {}
      extra: {}
      has_what: []
      host: 127.0.0.1
      last_seen: 1663953414.261237
      local_directory: /var/folders/68/bd5dqh4j3zgbwmhw2_9v4g180000gn/T/dask-worker-space/worker-zep12oa1
      long_running: []
      memory:
        managed: 0
        managed_in_memory: 0
        managed_spilled: 0
        optimistic: 194433024
        process: 194433024
        unmanaged: 194433024
        unmanaged_old: 194433024
        unmanaged_recent: 0
      memory_limit: 17179869184
      metrics:
        bandwidth:
          total: 100000000
          types: {}
          workers: {}
        cpu: 0.0
        event_loop_interval: 0.5
        executing: 0
        in_flight: 0
        in_memory: 0
        memory: 194433024
        num_fds: 25
        read_bytes: 0.0
        read_bytes_disk: 0.0
        ready: 0
        spilled_nbytes:
          disk: 0
          memory: 0
        time: 1663953414.226922
        write_bytes: 0.0
        write_bytes_disk: 0.0
      name: 0
      nanny: null
      nbytes: 0
      nthreads: 1
      occupancy: 0
      pid: 95840
      processing: {}
      resources: {}
      server_id: Worker-a9534fbd-86d5-428d-a260-ede2c284bea8
      services:
        dashboard: 50973
      status: '<Status.running: ''running''>'
      time_delay: 0.022827863693237305
      used_resources: {}
    tcp://127.0.0.1:50974:
      actors: []
      address: tcp://127.0.0.1:50974
      bandwidth: 100000000
      executing: {}
      extra: {}
      has_what: []
      host: 127.0.0.1
      last_seen: 1663953414.2626
      local_directory: /var/folders/68/bd5dqh4j3zgbwmhw2_9v4g180000gn/T/dask-worker-space/worker-bqrcff5y
      long_running: []
      memory:
        managed: 0
        managed_in_memory: 0
        managed_spilled: 0
        optimistic: 194506752
        process: 194506752
        unmanaged: 194506752
        unmanaged_old: 194506752
        unmanaged_recent: 0
      memory_limit: 17179869184
      metrics:
        bandwidth:
          total: 100000000
          types: {}
          workers: {}
        cpu: 0.0
        event_loop_interval: 0.5
        executing: 0
        in_flight: 0
        in_memory: 0
        memory: 194506752
        num_fds: 26
        read_bytes: 0.0
        read_bytes_disk: 0.0
        ready: 0
        spilled_nbytes:
          disk: 0
          memory: 0
        time: 1663953414.229425
        write_bytes: 0.0
        write_bytes_disk: 0.0
      name: 1
      nanny: null
      nbytes: 0
      nthreads: 2
      occupancy: 0
      pid: 95840
      processing: {}
      resources: {}
      server_id: Worker-44330199-ed16-48c6-b2c0-3554d0acd9c0
      services:
        dashboard: 50975
      status: '<Status.running: ''running''>'
      time_delay: 0.003255128860473633
      used_resources: {}
versions:
  host:
    LANG: None
    LC_ALL: None
    OS: Darwin
    OS-release: 21.5.0
    byteorder: little
    machine: x86_64
    processor: i386
    python: 3.9.12.final.0
    python-bits: 64
  packages:
    cloudpickle: 2.1.0
    dask: 2022.8.0
    distributed: 2022.8.0
    lz4: 4.0.0
    msgpack: 1.0.4
    numpy: 1.23.1
    pandas: 1.4.3
    python: 3.9.12.final.0
    toolz: 0.12.0
    tornado: '6.1'
workers:
  tcp://127.0.0.1:50972:
    address: tcp://127.0.0.1:50972
    busy_workers: []
    config:
      array:
        chunk-size: 128MiB
        rechunk-threshold: 4
        slicing:
          split-large-chunks: null
        svg:
          size: 120
      dataframe:
        parquet:
          metadata-task-size-local: 512
          metadata-task-size-remote: 16
        shuffle-compression: null
      distributed:
        adaptive:
          interval: 1s
          maximum: .inf
          minimum: 0
          target-duration: 5s
          wait-count: 3
        admin:
          event-loop: tornado
          log-format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
          log-length: 10000
          max-error-length: 10000
          pdb-on-err: false
          system-monitor:
            disk: true
            interval: 500ms
          tick:
            cycle: 1s
            interval: 500 ms
            limit: 3s
        client:
          heartbeat: 5s
          preload: []
          preload-argv: []
          scheduler-info-interval: 2s
          security-loader: null
        comm:
          compression: auto
          default-scheme: tcp
          offload: 10MiB
          recent-messages-log-length: 0
          require-encryption: null
          retry:
            count: 0
            delay:
              max: 20s
              min: 1s
          shard: 64MiB
          socket-backlog: 2048
          tcp:
            backend: tornado
          timeouts:
            connect: 5s
            tcp: 30s
          tls:
            ca-file: null
            ciphers: null
            client:
              cert: null
              key: null
            max-version: null
            min-version: 1.2
            scheduler:
              cert: null
              key: null
            worker:
              cert: null
              key: null
          ucx:
            create-cuda-context: null
            cuda-copy: null
            infiniband: null
            nvlink: null
            rdmacm: null
            tcp: null
          websockets:
            shard: 8MiB
          zstd:
            level: 3
            threads: 0
        dashboard:
          export-tool: false
          graph-max-items: 5000
          link: '{scheme}://{host}:{port}/status'
          prometheus:
            namespace: dask
        deploy:
          cluster-repr-interval: 500ms
          lost-worker-timeout: 15s
        diagnostics:
          computations:
            ignore-modules:
            - distributed
            - dask
            - xarray
            - cudf
            - cuml
            - prefect
            - xgboost
            max-history: 100
          erred-tasks:
            max-history: 100
          nvml: true
        nanny:
          environ:
            MALLOC_TRIM_THRESHOLD_: 65536
            MKL_NUM_THREADS: 1
            OMP_NUM_THREADS: 1
          preload: []
          preload-argv: []
        rmm:
          pool-size: null
        scheduler:
          active-memory-manager:
            interval: 2s
            policies:
            - class: distributed.active_memory_manager.ReduceReplicas
            start: false
          allowed-failures: 3
          allowed-imports:
          - dask
          - distributed
          bandwidth: 100000000
          blocked-handlers: []
          contact-address: null
          dashboard:
            bokeh-application:
              allow_websocket_origin:
              - '*'
              check_unused_sessions_milliseconds: 500
              keep_alive_milliseconds: 500
            status:
              task-stream-length: 1000
            tasks:
              task-stream-length: 100000
            tls:
              ca-file: null
              cert: null
              key: null
          default-data-size: 1kiB
          default-task-durations:
            rechunk-split: 1us
            split-shuffle: 1us
          events-cleanup-delay: 1h
          events-log-length: 100000
          http:
            routes:
            - distributed.http.scheduler.prometheus
            - distributed.http.scheduler.info
            - distributed.http.scheduler.json
            - distributed.http.health
            - distributed.http.proxy
            - distributed.http.statistics
          idle-timeout: null
          locks:
            lease-timeout: 30s
            lease-validation-interval: 10s
          pickle: true
          preload: []
          preload-argv: []
          transition-log-length: 100000
          unknown-task-duration: 500ms
          validate: false
          work-stealing: true
          work-stealing-interval: 100ms
          worker-ttl: 5 minutes
        version: 2
        worker:
          blocked-handlers: []
          connections:
            incoming: 10
            outgoing: 50
          daemon: true
          http:
            routes:
            - distributed.http.worker.prometheus
            - distributed.http.health
            - distributed.http.statistics
          lifetime:
            duration: null
            restart: false
            stagger: 0 seconds
          memory:
            max-spill: false
            monitor-interval: 100ms
            pause: 0.8
            rebalance:
              measure: optimistic
              recipient-max: 0.6
              sender-min: 0.3
              sender-recipient-gap: 0.1
            recent-to-old-time: 30s
            spill: 0.7
            target: 0.6
            terminate: 0.95
          multiprocessing-method: spawn
          preload: []
          preload-argv: []
          profile:
            cycle: 1000ms
            enabled: false
            interval: 10ms
            low-level: false
          resources: {}
          use-file-locking: true
          validate: false
      local_directory: /var/folders/68/bd5dqh4j3zgbwmhw2_9v4g180000gn/T
      optimization:
        fuse:
          active: null
          ave-width: 1
          max-depth-new-edges: null
          max-height: .inf
          max-width: null
          rename-keys: true
          subgraphs: null
      scheduler: dask.distributed
      shuffle: tasks
      temporary-directory: null
      tokenize:
        ensure-deterministic: false
      visualization:
        engine: null
    constrained: []
    data: {}
    data_needed: {}
    executing: []
    id: Worker-a9534fbd-86d5-428d-a260-ede2c284bea8
    in_flight_tasks: []
    in_flight_workers: {}
    incoming_transfer_log: []
    log: []
    logs: []
    long_running: []
    max_spill: false
    memory_limit: 17179869184
    memory_monitor_interval: 0.1
    memory_pause_fraction: 0.8
    memory_spill_fraction: 0.7
    memory_target_fraction: 0.6
    nthreads: 1
    outgoing_transfer_log: []
    ready: []
    running: true
    scheduler: tcp://127.0.0.1:50971
    status: '<Status.running: ''running''>'
    stimulus_log: []
    tasks: {}
    thread_id: 8633697792
    transition_counter: 0
    type: Worker
  tcp://127.0.0.1:50974:
    address: tcp://127.0.0.1:50974
    busy_workers: []
    config:
      array:
        chunk-size: 128MiB
        rechunk-threshold: 4
        slicing:
          split-large-chunks: null
        svg:
          size: 120
      dataframe:
        parquet:
          metadata-task-size-local: 512
          metadata-task-size-remote: 16
        shuffle-compression: null
      distributed:
        adaptive:
          interval: 1s
          maximum: .inf
          minimum: 0
          target-duration: 5s
          wait-count: 3
        admin:
          event-loop: tornado
          log-format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
          log-length: 10000
          max-error-length: 10000
          pdb-on-err: false
          system-monitor:
            disk: true
            interval: 500ms
          tick:
            cycle: 1s
            interval: 500 ms
            limit: 3s
        client:
          heartbeat: 5s
          preload: []
          preload-argv: []
          scheduler-info-interval: 2s
          security-loader: null
        comm:
          compression: auto
          default-scheme: tcp
          offload: 10MiB
          recent-messages-log-length: 0
          require-encryption: null
          retry:
            count: 0
            delay:
              max: 20s
              min: 1s
          shard: 64MiB
          socket-backlog: 2048
          tcp:
            backend: tornado
          timeouts:
            connect: 5s
            tcp: 30s
          tls:
            ca-file: null
            ciphers: null
            client:
              cert: null
              key: null
            max-version: null
            min-version: 1.2
            scheduler:
              cert: null
              key: null
            worker:
              cert: null
              key: null
          ucx:
            create-cuda-context: null
            cuda-copy: null
            infiniband: null
            nvlink: null
            rdmacm: null
            tcp: null
          websockets:
            shard: 8MiB
          zstd:
            level: 3
            threads: 0
        dashboard:
          export-tool: false
          graph-max-items: 5000
          link: '{scheme}://{host}:{port}/status'
          prometheus:
            namespace: dask
        deploy:
          cluster-repr-interval: 500ms
          lost-worker-timeout: 15s
        diagnostics:
          computations:
            ignore-modules:
            - distributed
            - dask
            - xarray
            - cudf
            - cuml
            - prefect
            - xgboost
            max-history: 100
          erred-tasks:
            max-history: 100
          nvml: true
        nanny:
          environ:
            MALLOC_TRIM_THRESHOLD_: 65536
            MKL_NUM_THREADS: 1
            OMP_NUM_THREADS: 1
          preload: []
          preload-argv: []
        rmm:
          pool-size: null
        scheduler:
          active-memory-manager:
            interval: 2s
            policies:
            - class: distributed.active_memory_manager.ReduceReplicas
            start: false
          allowed-failures: 3
          allowed-imports:
          - dask
          - distributed
          bandwidth: 100000000
          blocked-handlers: []
          contact-address: null
          dashboard:
            bokeh-application:
              allow_websocket_origin:
              - '*'
              check_unused_sessions_milliseconds: 500
              keep_alive_milliseconds: 500
            status:
              task-stream-length: 1000
            tasks:
              task-stream-length: 100000
            tls:
              ca-file: null
              cert: null
              key: null
          default-data-size: 1kiB
          default-task-durations:
            rechunk-split: 1us
            split-shuffle: 1us
          events-cleanup-delay: 1h
          events-log-length: 100000
          http:
            routes:
            - distributed.http.scheduler.prometheus
            - distributed.http.scheduler.info
            - distributed.http.scheduler.json
            - distributed.http.health
            - distributed.http.proxy
            - distributed.http.statistics
          idle-timeout: null
          locks:
            lease-timeout: 30s
            lease-validation-interval: 10s
          pickle: true
          preload: []
          preload-argv: []
          transition-log-length: 100000
          unknown-task-duration: 500ms
          validate: false
          work-stealing: true
          work-stealing-interval: 100ms
          worker-ttl: 5 minutes
        version: 2
        worker:
          blocked-handlers: []
          connections:
            incoming: 10
            outgoing: 50
          daemon: true
          http:
            routes:
            - distributed.http.worker.prometheus
            - distributed.http.health
            - distributed.http.statistics
          lifetime:
            duration: null
            restart: false
            stagger: 0 seconds
          memory:
            max-spill: false
            monitor-interval: 100ms
            pause: 0.8
            rebalance:
              measure: optimistic
              recipient-max: 0.6
              sender-min: 0.3
              sender-recipient-gap: 0.1
            recent-to-old-time: 30s
            spill: 0.7
            target: 0.6
            terminate: 0.95
          multiprocessing-method: spawn
          preload: []
          preload-argv: []
          profile:
            cycle: 1000ms
            enabled: false
            interval: 10ms
            low-level: false
          resources: {}
          use-file-locking: true
          validate: false
      local_directory: /var/folders/68/bd5dqh4j3zgbwmhw2_9v4g180000gn/T
      optimization:
        fuse:
          active: null
          ave-width: 1
          max-depth-new-edges: null
          max-height: .inf
          max-width: null
          rename-keys: true
          subgraphs: null
      scheduler: dask.distributed
      shuffle: tasks
      temporary-directory: null
      tokenize:
        ensure-deterministic: false
      visualization:
        engine: null
    constrained: []
    data: {}
    data_needed: {}
    executing: []
    id: Worker-44330199-ed16-48c6-b2c0-3554d0acd9c0
    in_flight_tasks: []
    in_flight_workers: {}
    incoming_transfer_log: []
    log: []
    logs: []
    long_running: []
    max_spill: false
    memory_limit: 17179869184
    memory_monitor_interval: 0.1
    memory_pause_fraction: 0.8
    memory_spill_fraction: 0.7
    memory_target_fraction: 0.6
    nthreads: 2
    outgoing_transfer_log: []
    ready: []
    running: true
    scheduler: tcp://127.0.0.1:50971
    status: '<Status.running: ''running''>'
    stimulus_log: []
    tasks: {}
    thread_id: 8633697792
    transition_counter: 0
    type: Worker
