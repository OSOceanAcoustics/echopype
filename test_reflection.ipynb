{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {},
      "outputs": [],
      "source": [
        "import dask_image.ndfilters\n",
        "import flox.xarray\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "import echopype as ep\n",
        "from echopype.clean.utils import (\n",
        "    extract_dB,\n",
        "    pool_Sv,\n",
        "    index_binning_pool_Sv,\n",
        "    index_binning_downsample_upsample_along_depth,\n",
        "    downsample_upsample_along_depth,\n",
        ")\n",
        "from echopype.utils.compute import _lin2log, _log2lin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "There is no file named JR161-D20061118-T010645.raw",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[175], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Open raw, calibrate, and add depth\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m ed \u001b[38;5;241m=\u001b[39m \u001b[43mep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mechopype/test_data/ek60/from_echopy/JR161-D20061118-T010645.raw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msonar_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEK60\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m ds_Sv \u001b[38;5;241m=\u001b[39m ep\u001b[38;5;241m.\u001b[39mcalibrate\u001b[38;5;241m.\u001b[39mcompute_Sv(ed)\n\u001b[0;32m      4\u001b[0m ds_Sv \u001b[38;5;241m=\u001b[39m ep\u001b[38;5;241m.\u001b[39mconsolidate\u001b[38;5;241m.\u001b[39madd_depth(ds_Sv)\n",
            "File \u001b[1;32mc:\\Users\\cmtug\\OneDrive\\Documents\\GitHub\\echopype\\echopype\\utils\\prov.py:237\u001b[0m, in \u001b[0;36madd_processing_level.<locals>.wrapper.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 237\u001b[0m     dataobj \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_echodata:\n\u001b[0;32m    239\u001b[0m         ed \u001b[38;5;241m=\u001b[39m dataobj\n",
            "File \u001b[1;32mc:\\Users\\cmtug\\OneDrive\\Documents\\GitHub\\echopype\\echopype\\convert\\api.py:455\u001b[0m, in \u001b[0;36mopen_raw\u001b[1;34m(raw_file, sonar_model, xml_path, include_bot, include_idx, convert_params, storage_options, use_swap, max_chunk_size)\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    451\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported echosounder model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msonar_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMust be one of: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(SONAR_MODELS)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m    452\u001b[0m     )\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# Check file extension and existence\u001b[39;00m\n\u001b[1;32m--> 455\u001b[0m file_chk, xml_chk, bot_chk, idx_chk \u001b[38;5;241m=\u001b[39m \u001b[43m_check_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msonar_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxml_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_bot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;66;03m# Parse raw file and organize data into groups\u001b[39;00m\n\u001b[0;32m    460\u001b[0m parser \u001b[38;5;241m=\u001b[39m SONAR_MODELS[sonar_model][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m](\n\u001b[0;32m    461\u001b[0m     file_chk,\n\u001b[0;32m    462\u001b[0m     \u001b[38;5;66;03m# Currently used only for AZFP XML File\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    468\u001b[0m     sonar_model\u001b[38;5;241m=\u001b[39msonar_model,\n\u001b[0;32m    469\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\cmtug\\OneDrive\\Documents\\GitHub\\echopype\\echopype\\convert\\api.py:340\u001b[0m, in \u001b[0;36m_check_file\u001b[1;34m(raw_file, sonar_model, xml_path, include_bot, include_idx, storage_options)\u001b[0m\n\u001b[0;32m    338\u001b[0m validate_ext \u001b[38;5;241m=\u001b[39m SONAR_MODELS[sonar_model][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidate_ext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fsmap\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mexists(fsmap\u001b[38;5;241m.\u001b[39mroot):\n\u001b[1;32m--> 340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere is no file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPath(raw_file)\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    341\u001b[0m validate_ext(Path(raw_file)\u001b[38;5;241m.\u001b[39msuffix\u001b[38;5;241m.\u001b[39mupper())\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(raw_file), \u001b[38;5;28mstr\u001b[39m(xml), bot_file, idx_file\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: There is no file named JR161-D20061118-T010645.raw"
          ]
        }
      ],
      "source": [
        "# Open raw, calibrate, and add depth\n",
        "ed = ep.open_raw(\"echopype/test_data/ek60/from_echopy/JR161-D20061118-T010645.raw\", sonar_model=\"EK60\")\n",
        "ds_Sv = ep.calibrate.compute_Sv(ed)\n",
        "ds_Sv = ep.consolidate.add_depth(ds_Sv)\n",
        "\n",
        "# Select Sv subset\n",
        "ds_Sv = ds_Sv.isel(ping_time=slice(100,120), range_sample=slice(990,1020))\n",
        "\n",
        "# Set window args\n",
        "depth_bin = 1\n",
        "num_side_pings = 2\n",
        "exclude_above = 186\n",
        "range_var = \"depth\"\n",
        "chunk_dict = {}\n",
        "\n",
        "# Compute pooled Sv using index binning\n",
        "pooled_Sv = index_binning_pool_Sv(\n",
        "    ds_Sv, np.nanmean, depth_bin, num_side_pings, exclude_above, range_var, chunk_dict\n",
        ").compute()\n",
        "\n",
        "# Compute `ds_Sv` prior to using it for manual testing\n",
        "ds_Sv = ds_Sv.compute()\n",
        "\n",
        "# Iterate through channels\n",
        "for channel_index in range(len(ds_Sv[\"channel\"])):\n",
        "    # Grab single channel Sv\n",
        "    chan_ds_Sv = ds_Sv.isel(channel=channel_index)\n",
        "\n",
        "    # Compute number of range sample indices that are needed to encapsulate the `depth_bin`\n",
        "    # value per channel.\n",
        "    num_range_sample_indices = np.ceil(\n",
        "        depth_bin / np.nanmean(np.diff(chan_ds_Sv[\"depth\"], axis=1), axis=(0,1))\n",
        "    ).astype(int)\n",
        "\n",
        "    # Compute min and max values\n",
        "    range_sample_min = ds_Sv[\"range_sample\"].min()\n",
        "    range_sample_max = ds_Sv[\"range_sample\"].max()\n",
        "    ping_time_index_min = 0\n",
        "    ping_time_index_max = len(ds_Sv[\"ping_time\"])\n",
        "\n",
        "    # Create ping time indices array\n",
        "    ping_time_indices = xr.DataArray(\n",
        "        np.arange(len(chan_ds_Sv[\"ping_time\"]), dtype=int),\n",
        "        dims=[\"ping_time\"],\n",
        "        coords=[ds_Sv[\"ping_time\"]],\n",
        "        name=\"ping_time_indices\",\n",
        "    )\n",
        "\n",
        "    # Check correct binning and aggregation values\n",
        "    for ping_time_index in range(len(chan_ds_Sv[\"ping_time\"])):\n",
        "        for range_sample_index in range(len(chan_ds_Sv[\"range_sample\"])):\n",
        "            # Grab pooled value\n",
        "            pooled_value = pooled_Sv.isel(\n",
        "                channel=channel_index,\n",
        "                ping_time=ping_time_index,\n",
        "                range_sample=range_sample_index\n",
        "            ).data\n",
        "            if not np.isnan(pooled_value):\n",
        "                # Check that manually computed pool value matches Dask-Image's\n",
        "                # generic filter output\n",
        "                assert np.isclose(\n",
        "                    pooled_value,\n",
        "                    _lin2log(\n",
        "                        _log2lin(\n",
        "                            ds_Sv[\"Sv\"].isel(\n",
        "                                channel=channel_index,\n",
        "                                ping_time=slice(\n",
        "                                    ping_time_index - num_side_pings,\n",
        "                                    ping_time_index + 1 + num_side_pings\n",
        "                                ),\n",
        "                                range_sample=slice(\n",
        "                                    range_sample_index - num_range_sample_indices,\n",
        "                                    range_sample_index + 1 + num_range_sample_indices\n",
        "                                )\n",
        "                            )\n",
        "                        ).pipe(np.nanmedian)\n",
        "                    ),\n",
        "                    rtol=1e-10,\n",
        "                    atol=1e-10,\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Shape\n",
        "array_shape = [6,6]\n",
        "convolve_shape = [5,5]\n",
        "radius_tuple = [\n",
        "    convolve_shape[0]//2,\n",
        "    convolve_shape[1]//2\n",
        "]\n",
        "\n",
        "# Create mock data\n",
        "mock_data = np.random.rand(array_shape[0], array_shape[1])\n",
        "da_mock_data = xr.DataArray(\n",
        "    mock_data,\n",
        "    dims=(\"row\", \"col\"),\n",
        "    name=\"random_numbers\"\n",
        ").chunk(\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Symmetric matches the reflect logic in Dask Generic Filter\n",
        "data_padded_with_reflection = np.pad(\n",
        "    mock_data,\n",
        "    radius_tuple,\n",
        "    mode='symmetric'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use Echopype to calculate channel\n",
        "output = dask_image.ndfilters.generic_filter(\n",
        "    image=da_mock_data.data,\n",
        "    function=np.nanmean,\n",
        "    size=convolve_shape,\n",
        "    mode=\"reflect\",\n",
        ").compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create mock data\n",
        "padded_reflection_shape = data_padded_with_reflection.shape\n",
        "output_expected = np.full(mock_data.shape, np.nan)\n",
        "\n",
        "# Iterate through value\n",
        "for i in range(radius_tuple[0], padded_reflection_shape[0] - radius_tuple[0]):\n",
        "    for j in range(radius_tuple[1], padded_reflection_shape[1] - radius_tuple[1]):\n",
        "        # Create window on data padded with reflection\n",
        "        window = data_padded_with_reflection[\n",
        "            (i-radius_tuple[0]):(i+radius_tuple[0] + 1),\n",
        "            (j-radius_tuple[1]):(j+radius_tuple[1] + 1)\n",
        "        ]\n",
        "\n",
        "        # Calculate aggregate value of window\n",
        "        agg_value = np.nanmean(window)\n",
        "\n",
        "        # Place in output_expected\n",
        "        output_expected[\n",
        "            i-radius_tuple[0],\n",
        "            j-radius_tuple[1]\n",
        "        ] = agg_value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 172,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.allclose(output, output_expected)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "echopype",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
